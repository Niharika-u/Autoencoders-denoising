{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV8FmEOc-m7e",
        "outputId": "35131d80-0c22-48db-f159-cff124e9c87b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.11.0\n",
            "Is Executing Eagerly? True\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "#try:\n",
        "  # The %tensorflow_version magic only works in colab.\n",
        "  # tensorflow_version 2.x\n",
        "#except Exception:\n",
        "#  pass\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "print('Is Executing Eagerly?', tf.executing_eagerly())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZNJvrdd6Ty5"
      },
      "outputs": [],
      "source": [
        "def display_test_images(x_test, decoded_imgs):\n",
        "\n",
        "    n = 10\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for i in range(1, n + 1):\n",
        "        # Display original\n",
        "        ax = plt.subplot(2, n, i)\n",
        "        plt.imshow(x_test[i].reshape(28, 28))\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "        # Display reconstruction\n",
        "        ax = plt.subplot(2, n, i + n)\n",
        "        plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_plot(model_history, title):\n",
        "    training_loss = model_history.history['loss']\n",
        "    test_loss = model_history.history['val_loss']\n",
        "    epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "    plt.plot(epoch_count, training_loss, 'r--')\n",
        "    plt.plot(epoch_count, test_loss, 'b-')\n",
        "    plt.legend(['Training Loss', 'Test Loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(title)\n",
        "    plt.show();"
      ],
      "metadata": {
        "id": "dk-5BQmD9Xpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2gRgd787gyp"
      },
      "outputs": [],
      "source": [
        "def plot_loss(model):\n",
        "    plt.plot(range(10), model.loss)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHv80Luz-_Ed"
      },
      "source": [
        "# Task1: Dense Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41ymX79n5GY2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6a2b2c-7bc3-4a6d-bef7-180e9f22fc33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf15XyiC5OE7",
        "outputId": "de450ba4-8b25-4a61-e671-5026afe34755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ],
      "source": [
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX63Fy_E67pd"
      },
      "outputs": [],
      "source": [
        "input_img = keras.Input(shape=(784,))\n",
        "encoded = keras.layers.Dense(128, activation='relu')(input_img)\n",
        "\n",
        "decoded = keras.layers.Dense(128, activation='relu')(encoded)\n",
        "decoded = keras.layers.Dense(784, activation='sigmoid')(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGL6bkLL7UCk"
      },
      "outputs": [],
      "source": [
        "def loss(preds, real):\n",
        "  return tf.reduce_mean(tf.square(tf.subtract(preds, real)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8id2Kx87IXN",
        "outputId": "6f8f6bb6-05be-4c4b-d488-2d248d898483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 784)               101136    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 218,128\n",
            "Trainable params: 218,128\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "autoencoder1 = keras.Model(input_img, decoded)\n",
        "autoencoder1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2), loss=loss)\n",
        "autoencoder1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELPES-z97wps",
        "outputId": "480c1c35-e943-40a4-f94a-8fe3b741402d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "235/235 [==============================] - 12s 12ms/step - loss: 0.0288 - val_loss: 0.0114\n",
            "Epoch 2/100\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.0096 - val_loss: 0.0080\n",
            "Epoch 3/100\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.0075 - val_loss: 0.0065\n",
            "Epoch 4/100\n",
            "235/235 [==============================] - 2s 6ms/step - loss: 0.0065 - val_loss: 0.0060\n",
            "Epoch 5/100\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0058 - val_loss: 0.0056\n",
            "Epoch 6/100\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0055 - val_loss: 0.0052\n",
            "Epoch 7/100\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0053 - val_loss: 0.0052\n",
            "Epoch 8/100\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.0052 - val_loss: 0.0049\n",
            "Epoch 9/100\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.0051 - val_loss: 0.0051\n",
            "Epoch 10/100\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.0050 - val_loss: 0.0049\n",
            "Epoch 11/100\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.0050 - val_loss: 0.0050\n",
            "Epoch 12/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0050 - val_loss: 0.0048\n",
            "Epoch 13/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0049 - val_loss: 0.0049\n",
            "Epoch 14/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0049 - val_loss: 0.0048\n",
            "Epoch 15/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0049 - val_loss: 0.0049\n",
            "Epoch 16/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0049 - val_loss: 0.0049\n",
            "Epoch 17/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0048 - val_loss: 0.0048\n",
            "Epoch 18/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0048 - val_loss: 0.0049\n",
            "Epoch 19/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0048 - val_loss: 0.0048\n",
            "Epoch 20/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0048 - val_loss: 0.0048\n",
            "Epoch 21/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0048 - val_loss: 0.0049\n",
            "Epoch 22/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0048 - val_loss: 0.0048\n",
            "Epoch 23/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0048 - val_loss: 0.0047\n",
            "Epoch 24/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0048 - val_loss: 0.0047\n",
            "Epoch 25/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0048 - val_loss: 0.0047\n",
            "Epoch 26/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0048 - val_loss: 0.0048\n",
            "Epoch 27/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0048 - val_loss: 0.0047\n",
            "Epoch 28/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0047 - val_loss: 0.0048\n",
            "Epoch 29/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0047 - val_loss: 0.0049\n",
            "Epoch 30/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0047 - val_loss: 0.0046\n",
            "Epoch 31/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0047 - val_loss: 0.0047\n",
            "Epoch 32/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0047 - val_loss: 0.0046\n",
            "Epoch 33/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0046 - val_loss: 0.0047\n",
            "Epoch 34/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0046 - val_loss: 0.0046\n",
            "Epoch 35/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0046 - val_loss: 0.0047\n",
            "Epoch 36/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0047 - val_loss: 0.0047\n",
            "Epoch 37/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0046 - val_loss: 0.0046\n",
            "Epoch 38/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0046 - val_loss: 0.0047\n",
            "Epoch 39/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0046 - val_loss: 0.0047\n",
            "Epoch 40/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0046 - val_loss: 0.0046\n",
            "Epoch 41/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0046 - val_loss: 0.0049\n",
            "Epoch 42/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0046 - val_loss: 0.0046\n",
            "Epoch 43/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0046 - val_loss: 0.0046\n",
            "Epoch 44/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0046 - val_loss: 0.0047\n",
            "Epoch 45/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0046 - val_loss: 0.0045\n",
            "Epoch 46/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0046 - val_loss: 0.0047\n",
            "Epoch 47/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0046 - val_loss: 0.0048\n",
            "Epoch 48/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0046 - val_loss: 0.0045\n",
            "Epoch 49/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0046 - val_loss: 0.0047\n",
            "Epoch 50/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0046 - val_loss: 0.0047\n",
            "Epoch 51/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0046 - val_loss: 0.0045\n",
            "Epoch 52/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 53/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0046 - val_loss: 0.0045\n",
            "Epoch 54/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0046 - val_loss: 0.0045\n",
            "Epoch 55/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0046 - val_loss: 0.0045\n",
            "Epoch 56/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0045 - val_loss: 0.0047\n",
            "Epoch 57/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0046 - val_loss: 0.0045\n",
            "Epoch 58/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0045 - val_loss: 0.0045\n",
            "Epoch 59/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0045 - val_loss: 0.0044\n",
            "Epoch 60/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0045 - val_loss: 0.0045\n",
            "Epoch 61/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0045 - val_loss: 0.0044\n",
            "Epoch 62/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0045 - val_loss: 0.0045\n",
            "Epoch 63/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0045 - val_loss: 0.0046\n",
            "Epoch 64/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0045 - val_loss: 0.0047\n",
            "Epoch 65/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0045 - val_loss: 0.0046\n",
            "Epoch 66/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0045 - val_loss: 0.0046\n",
            "Epoch 67/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0045 - val_loss: 0.0045\n",
            "Epoch 68/100\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0044\n",
            "Epoch 69/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 70/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 71/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 72/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0044 - val_loss: 0.0045\n",
            "Epoch 73/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 74/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 75/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0046\n",
            "Epoch 76/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 77/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 78/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 79/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0047\n",
            "Epoch 80/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 81/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 82/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 83/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 84/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 85/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 86/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0045\n",
            "Epoch 87/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0044 - val_loss: 0.0046\n",
            "Epoch 88/100\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 89/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 90/100\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 91/100\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 92/100\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 93/100\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 94/100\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 95/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 96/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 97/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 98/100\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 99/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0043 - val_loss: 0.0044\n",
            "Epoch 100/100\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0044\n"
          ]
        }
      ],
      "source": [
        "history1 = autoencoder1.fit(x_train, x_train,\n",
        "                epochs=100,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),\n",
        "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder1')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "sGzrhhVX75Bt",
        "outputId": "70657d67-cc2c-4615-88fe-ec2c21bf46a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "decoded_images = autoencoder1.predict(x_test)\n",
        "display_test_images(x_test, decoded_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELN2dspt6CKA"
      },
      "source": [
        "# Custom Defined Dense Architecture 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-S4doauAXFJ"
      },
      "outputs": [],
      "source": [
        "input_img = keras.Input(shape=(784,))\n",
        "encoded = keras.layers.Dense(128, activation='relu')(input_img)\n",
        "encoded = keras.layers.BatchNormalization()(encoded)\n",
        "encoded = keras.layers.Dense(64, activation='relu')(encoded)\n",
        "encoded = keras.layers.BatchNormalization()(encoded)\n",
        "\n",
        "decoded = keras.layers.Dense(64, activation='relu')(encoded)\n",
        "encoded = keras.layers.BatchNormalization()(decoded)\n",
        "decoded = keras.layers.Dense(128, activation='relu')(decoded)\n",
        "encoded = keras.layers.BatchNormalization()(decoded)\n",
        "decoded = keras.layers.Dense(784, activation='sigmoid')(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4xEUztjAwKz",
        "outputId": "eeacedbf-e27a-4f08-d118-cd6623a46f17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               100480    \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 128)              512       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               8320      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 784)               101136    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 223,120\n",
            "Trainable params: 222,736\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "autoencoder2 = keras.Model(input_img, decoded)\n",
        "autoencoder2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2), loss=loss)\n",
        "autoencoder2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dmHp4-909ol",
        "outputId": "05e8e278-8e85-4b5a-acba-3847cff57097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "235/235 [==============================] - 4s 6ms/step - loss: 0.0294 - val_loss: 0.0282\n",
            "Epoch 2/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0151 - val_loss: 0.0155\n",
            "Epoch 3/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0125 - val_loss: 0.0122\n",
            "Epoch 4/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0111 - val_loss: 0.0118\n",
            "Epoch 5/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0102 - val_loss: 0.0100\n",
            "Epoch 6/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0095 - val_loss: 0.0094\n",
            "Epoch 7/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0090 - val_loss: 0.0092\n",
            "Epoch 8/100\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.0085 - val_loss: 0.0082\n",
            "Epoch 9/100\n",
            "235/235 [==============================] - 2s 6ms/step - loss: 0.0080 - val_loss: 0.0077\n",
            "Epoch 10/100\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0077 - val_loss: 0.0075\n",
            "Epoch 11/100\n",
            "233/235 [============================>.] - ETA: 0s - loss: 0.0072"
          ]
        }
      ],
      "source": [
        "history2 = autoencoder2.fit(x_train, x_train,\n",
        "                epochs=100,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxUcHhG-Aybz"
      },
      "outputs": [],
      "source": [
        "decoded_images = autoencoder2.predict(x_test)\n",
        "display_test_images(x_test, decoded_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgS4tb9cCX7e"
      },
      "source": [
        "# Custom defined Dense Architecture 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gcbnj2z9OZY"
      },
      "outputs": [],
      "source": [
        "input_img = keras.Input(shape=(784,))\n",
        "encoded = keras.layers.Dense(256, activation='relu')(input_img)\n",
        "\n",
        "decoded = keras.layers.Dense(256, activation='relu')(encoded)\n",
        "decoded = keras.layers.Dense(784, activation='sigmoid')(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0eWmZts9OL7"
      },
      "outputs": [],
      "source": [
        "autoencoder3 = keras.Model(input_img, decoded)\n",
        "autoencoder3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2), loss=loss)\n",
        "autoencoder3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQLD-51i9N9H"
      },
      "outputs": [],
      "source": [
        "history3 = autoencoder3.fit(x_train, x_train,\n",
        "                epochs=100,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yryE-Kt99W3X"
      },
      "outputs": [],
      "source": [
        "decoded_images = autoencoder3.predict(x_test)\n",
        "display_test_images(x_test, decoded_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-mBXkPLDL4V"
      },
      "source": [
        "# Basic CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQetQuaDDW7p"
      },
      "outputs": [],
      "source": [
        "input_img = tf.keras.layers.Input(shape=(28, 28, 1)) # adapt this if using `channels_first` image data format\n",
        "\n",
        "x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
        "\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder4 = tf.keras.models.Model(input_img, decoded)\n",
        "autoencoder4.compile(optimizer='adadelta', loss='binary_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkexGlRcDXh8"
      },
      "outputs": [],
      "source": [
        "# to train this model we will with original MNIST digits with shape (samples, 3, 28, 28) and we will just normalize pixel values between 0 and 1\n",
        "# (x_train, _), (x_test, _) = load_data('../input/mnist.npz')\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7hJOPkDDjSa"
      },
      "outputs": [],
      "source": [
        "history4 = autoencoder4.fit(x_train, x_train, epochs=100, batch_size=128,\n",
        "                shuffle=True, validation_data=(x_test, x_test),\n",
        "                callbacks=[tf.keras.callbacks.TensorBoard(log_dir='./tmp/autoencoder')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK2UhsDI-Dhr"
      },
      "outputs": [],
      "source": [
        "decoded_images = autoencoder4.predict(x_test)\n",
        "display_test_images(x_test, decoded_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwc_6a57Dw-7"
      },
      "source": [
        "# Complex CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTVE_fAwARFV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMiN3Rq-D0Rv"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "\n",
        "input_img = tf.keras.layers.Input(shape=(28, 28, 1)) # adapt this if using `channels_first` image data format\n",
        "\n",
        "encoded = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(input_img)\n",
        "encoded = BatchNormalization()(encoded)\n",
        "encoded = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = BatchNormalization()(encoded)\n",
        "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(encoded)\n",
        "encoded = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = BatchNormalization()(encoded)\n",
        "encoded = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = BatchNormalization()(encoded)\n",
        "encoded = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = BatchNormalization()(encoded)\n",
        "encoded = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
        "\n",
        "# print(encoded)\n",
        "\n",
        "decoded = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
        "decoded = BatchNormalization()(decoded)\n",
        "decoded = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = BatchNormalization()(decoded)\n",
        "decoded = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = BatchNormalization()(decoded)\n",
        "decoded = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = BatchNormalization()(decoded)\n",
        "\n",
        "decoded = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = BatchNormalization()(decoded)\n",
        "decoded = tf.keras.layers.UpSampling2D((2, 2))(decoded)\n",
        "decoded = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = BatchNormalization()(decoded)\n",
        "\n",
        "\n",
        "decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(decoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD6BQ4rJ_CxI"
      },
      "outputs": [],
      "source": [
        "autoencoder5 = tf.keras.models.Model(input_img, decoded)\n",
        "autoencoder5.compile(optimizer='adam', loss='mse', metrics=[RootMeanSquaredError()])\n",
        "autoencoder5.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayU1sWxBGDOQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azlZH3LAGDmK"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_path = \"improved_cnn_section1.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='max')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlzOiVxdGH86"
      },
      "outputs": [],
      "source": [
        "\n",
        "history5 = autoencoder5.fit(x_train, x_train, epochs=100, batch_size=128,\n",
        "                shuffle=True, validation_data=(x_test, x_test),\n",
        "                callbacks=[tf.keras.callbacks.TensorBoard(log_dir='./tmp/autoencoder'), checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN6yVbVUHwnV"
      },
      "outputs": [],
      "source": [
        "decoded_images = autoencoder5.predict(x_test)\n",
        "display_test_images(x_test, decoded_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y7hhAl4B38F"
      },
      "source": [
        "# Summary of above"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Model 1 = Dense 128, Adam, le-2, loss 0.0043\n",
        "2. Model 2 = Dense 128, 64, Adam, le-2, loss 0.0041\n",
        "3. Model 3 = Dense 256, Adam, le-2 loss 0.0034\n",
        "4. Model 4 = Basic CNN - 16, 8, 8, Maxpool, adadelta,\n",
        "crossentropy, loss 0.19\n",
        "5. Model 5 = CNN 128, 64, 32, Maxpool, Batch normalization , adam, loss = 0.0042"
      ],
      "metadata": {
        "id": "8C2WcM59-0-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above experiments, for simple image reconstruction:\n",
        "1. only one Dense layer is enough for a good accuracy. 2. The accuracy did not improve with adding more layers to Dense network.\n",
        "3. Increasing the neurons from 128 to 256 in Dense layer has slightly improved the accuracy but increasing even more to 512 did not change anything\n",
        "4. The CNN's perform similar to dense since it is a simple image reconstruction and even dense networks can work better as there is not a lot of spatial information.\n",
        "5. Simple CNN will of course not work better as number of filters are very less.\n",
        "6. Increasing the filters, adding Maxpool and batchnormalizing has given siliar results to that of a dense model.\n",
        "7. Also from the graph we can see that complex CNN in fact dont need a lot of epochs to get trained\n",
        "8. Adam optimizers are better than adadelta"
      ],
      "metadata": {
        "id": "zl2ExlUfWj0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_plot(history1, \"Dense 128, Adam, le-2, loss 0.0043\")\n",
        "loss_plot(history2, \"Dense 128, 64, Adam, le-2, loss 0.0041\")\n",
        "loss_plot(history3, \"Dense 256, Adam, le-2, loss 0.0034\")\n",
        "loss_plot(history4, \"Basic CNN - 16, 8, 8, Maxpool, adadelta, crossentropy, loss 0.19\")\n",
        "loss_plot(history5, \"CNN 128, 64, 32, Maxpool, Batch normalization , adam, loss = 0.0042\")"
      ],
      "metadata": {
        "id": "bytpL4-z-iv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom dense Architecture 1"
      ],
      "metadata": {
        "id": "JeP6xkqGKHnn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUZoyZ6JCCL_"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "noise_factor = 0.5\n",
        "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL-z4az4CiGW"
      },
      "outputs": [],
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 2))\n",
        "for i in range(1, n + 1):\n",
        "    ax = plt.subplot(1, n, i)\n",
        "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glKMFX3nCwoi"
      },
      "outputs": [],
      "source": [
        "input_img = keras.Input(shape=(784,))\n",
        "encoded = keras.layers.Dense(128, activation='relu')(input_img)\n",
        "encoded = keras.layers.Dense(64, activation='relu')(encoded)\n",
        "encoded = keras.layers.BatchNormalization()(encoded)\n",
        "\n",
        "decoded = keras.layers.Dense(128, activation='relu')(encoded)\n",
        "decoded = keras.layers.BatchNormalization()(decoded)\n",
        "decoded = keras.layers.Dense(784, activation='sigmoid')(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2y14j_yDrLI"
      },
      "outputs": [],
      "source": [
        "autoencoder6 = keras.Model(input_img, decoded)\n",
        "autoencoder6.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2), loss=loss)\n",
        "autoencoder6.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzXlZWooD2pw"
      },
      "outputs": [],
      "source": [
        "history6 = autoencoder6.fit(x_train_noisy, x_train,\n",
        "                epochs=100,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test_noisy, x_test),\n",
        "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder1')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2rFF-U9D9E_"
      },
      "outputs": [],
      "source": [
        "decoded_images = autoencoder6.predict(x_test_noisy)\n",
        "display_test_images(x_test_noisy, decoded_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFsKm_6I_t1Q"
      },
      "source": [
        "# Custom Dense Architecture 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzNbgOCrFIAM"
      },
      "outputs": [],
      "source": [
        "input_img = keras.Input(shape=(784,))\n",
        "encoded = keras.layers.Dense(256, activation='relu')(input_img)\n",
        "encoded = keras.layers.Dense(128, activation='relu')(encoded)\n",
        "encoded = keras.layers.BatchNormalization()(encoded)\n",
        "\n",
        "decoded = keras.layers.Dense(128, activation='relu')(encoded)\n",
        "decoded = keras.layers.Dense(256, activation='relu')(decoded)\n",
        "decoded = keras.layers.BatchNormalization()(decoded)\n",
        "decoded = keras.layers.Dense(784, activation='sigmoid')(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNncRu92FQ8T"
      },
      "outputs": [],
      "source": [
        "autoencoder7 = keras.Model(input_img, decoded)\n",
        "autoencoder7.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2), loss=loss)\n",
        "autoencoder7.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHfmMRUnFZMI"
      },
      "outputs": [],
      "source": [
        "history7 = autoencoder7.fit(x_train_noisy, x_train,\n",
        "                epochs=100,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test_noisy, x_test),\n",
        "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder1')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcAwd4zNFieb"
      },
      "outputs": [],
      "source": [
        "decoded_images = autoencoder7.predict(x_test_noisy)\n",
        "display_test_images(x_test_noisy, decoded_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcyred_e_yIM"
      },
      "source": [
        "# Custom Simple CNN Architecture 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4E7oeYh2GBZs"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
        "\n",
        "noise_factor = 0.5\n",
        "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZW-dxycF7Id"
      },
      "outputs": [],
      "source": [
        "input_img = tf.keras.layers.Input(shape=(28, 28, 1)) # adapt this if using `channels_first` image data format\n",
        "\n",
        "x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
        "\n",
        "x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(x)\n",
        "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder8 = tf.keras.models.Model(input_img, decoded)\n",
        "autoencoder8.compile(optimizer='adadelta', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Htc75Cm9GFjl"
      },
      "outputs": [],
      "source": [
        "history8 = autoencoder8.fit(x_train_noisy, x_train, epochs=100, batch_size=128,\n",
        "                shuffle=True, validation_data=(x_test_noisy, x_test),\n",
        "                callbacks=[tf.keras.callbacks.TensorBoard(log_dir='./tmp/autoencoder')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yups3KVbGMR_"
      },
      "outputs": [],
      "source": [
        "decoded_images = autoencoder8.predict(x_test_noisy)\n",
        "display_test_images(x_test_noisy, decoded_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmr2lMeb_9mQ"
      },
      "source": [
        "# Custom Complex CNN Architecture 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y97fA6mGx_5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "\n",
        "input_img = tf.keras.layers.Input(shape=(28, 28, 1)) # adapt this if using `channels_first` image data format\n",
        "\n",
        "encoded = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(input_img)\n",
        "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(encoded)\n",
        "encoded = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = BatchNormalization()(encoded)\n",
        "encoded = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
        "\n",
        "# print(encoded)\n",
        "\n",
        "decoded = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
        "decoded = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = BatchNormalization()(decoded)\n",
        "decoded = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = tf.keras.layers.UpSampling2D((2, 2))(decoded)\n",
        "decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(decoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOVqGENfHftk"
      },
      "outputs": [],
      "source": [
        "autoencoder9 = tf.keras.models.Model(input_img, decoded)\n",
        "autoencoder9.compile(optimizer='adam', loss='mse', metrics=[RootMeanSquaredError()])\n",
        "autoencoder9.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm4d54oaHoao"
      },
      "outputs": [],
      "source": [
        "history9 = autoencoder9.fit(x_train_noisy, x_train, epochs=100, batch_size=128,\n",
        "                shuffle=True, validation_data=(x_test_noisy, x_test),\n",
        "                callbacks=[tf.keras.callbacks.TensorBoard(log_dir='./tmp/autoencoder')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOyWgfadIBSK"
      },
      "outputs": [],
      "source": [
        "decoded_images = autoencoder9.predict(x_test_noisy)\n",
        "display_test_images(x_test_noisy, decoded_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary of section 2"
      ],
      "metadata": {
        "id": "E40M-mBSAYIT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XEvUlYdAKU2"
      },
      "source": [
        "1. Model 6 = Dense 128, 64, 128, Batch Norm Adam, le-2, loss 0.014\n",
        "2. Model 7 = Dense 256, 128, Batch Norm, Adam, loss 0.012\n",
        "3. Model 8 = Basic CNN - 16, 8, 8, Maxpool, adadelta,\n",
        "crossentropy, 0.05\n",
        "4. Model 9 = 128, 64, 32, Maxpool, batch norm, 0.09"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. CNNs have an advantage over dense neural networks due to their ability to learn spatial patterns in the data\n",
        "2. We can see that CNNs are more effective than dense neural networks for handling noisy data because they use pooling layers to reduce the effects of noise in the data.\n",
        "So for this kind of data which is complex non-linear, dense networks may not be of great help"
      ],
      "metadata": {
        "id": "qOpwnmyjiboX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_plot(history6, \"Dense 128, 64, 128, Batch Norm Adam, le-2, loss 0.014\")\n",
        "loss_plot(history7, \"256, 128, Batch Norm, Adam, loss 0.012\")\n",
        "loss_plot(history8, \"Basic CNN - 16, 8, 8, Maxpool, adadelta, crossentropy, 0.05\")\n",
        "loss_plot(history9, \"128, 64, 32, Maxpool, batch norm, 0.0069\")"
      ],
      "metadata": {
        "id": "k_jf8j0aAhiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqBlaeHrI6CR"
      },
      "source": [
        "# Text Reconstruction Dense model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pU41bbYhXTR"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "\n",
        "# Do some code, e.g. train and save model\n",
        "\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fMgOHpAesG6"
      },
      "outputs": [],
      "source": [
        "def print_recontructed_text(X_test, predicted_label):\n",
        "\n",
        "    for i in range(4):\n",
        "      f, ax = plt.subplots(1,2, figsize=(10,8))\n",
        "      ax[0].imshow(np.squeeze(X_test[i]), cmap='gray')\n",
        "      ax[1].imshow(predicted_label[i], cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP2PQOJiGLhA"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.io import imread, imshow, imsave\n",
        "from tensorflow.keras.preprocessing.image import load_img, array_to_img, img_to_array\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Input\n",
        "from keras.optimizers import SGD, Adam, Adadelta, Adagrad\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "np.random.seed(111)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRT4atMncQEZ"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()\n",
        "g_auth = GoogleAuth()\n",
        "g_auth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(g_auth)\n",
        "drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLFwSmwfcSNW"
      },
      "outputs": [],
      "source": [
        "file_id = drive.ListFile({'q':\"title='train.zip'\"}).GetList()[0]\n",
        "f1 = drive.CreateFile({'id': file_id['id']})\n",
        "f1.GetContentFile('train.zip')\n",
        "\n",
        "file_id = drive.ListFile({'q':\"title='train_cleaned.zip'\"}).GetList()[0]\n",
        "f2 = drive.CreateFile({'id': file_id['id']})\n",
        "f2.GetContentFile('train_cleaned.zip')\n",
        "\n",
        "file_id = drive.ListFile({'q':\"title='test.zip'\"}).GetList()[0]\n",
        "f3 = drive.CreateFile({'id': file_id['id']})\n",
        "f3.GetContentFile('test.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDhpEs23cU0E"
      },
      "outputs": [],
      "source": [
        "!unzip train.zip\n",
        "!unzip train_cleaned.zip\n",
        "!unzip test.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg-SI1ARcZTC"
      },
      "outputs": [],
      "source": [
        "path_files = '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhRYN5QKcbg0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "train_directory = os.path.join(path_files,\n",
        "                         'train')\n",
        "train_cleaned_directory = os.path.join(path_files,\n",
        "                         'train_cleaned')\n",
        "test_directory = os.path.join(path_files,\n",
        "                        'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOhs1DkIcdzd"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "Y = []\n",
        "X_test = []\n",
        "\n",
        "for img in os.listdir(train_directory):\n",
        "    img = load_img(train_directory + \"/\" + img, color_mode = \"grayscale\",target_size=(420,540))\n",
        "    img = img_to_array(img).astype('float32')/255.\n",
        "    X.append(img)\n",
        "\n",
        "for img in os.listdir(train_cleaned_directory):\n",
        "    img = load_img(train_cleaned_directory + \"/\" + img, color_mode = \"grayscale\",target_size=(420,540))\n",
        "    img = img_to_array(img).astype('float32')/255.\n",
        "    Y.append(img)\n",
        "\n",
        "for img in os.listdir(test_directory):\n",
        "    img = load_img(test_directory + \"/\" + img, color_mode = \"grayscale\",target_size=(420,540))\n",
        "    img = img_to_array(img).astype('float32')/255.\n",
        "    X_test.append(img)\n",
        "    if len(X_test) == 4:\n",
        "      break\n",
        "\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "# Split the dataset into training and validation. Always set the random state!!\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.1, random_state=111)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDZuoAxPc7yL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "import keras\n",
        "\n",
        "input_img = tf.keras.layers.Input(shape=(226800))\n",
        "encoder = tf.keras.layers.Dense(128, activation='relu')(input_img)\n",
        "encoder = tf.keras.layers.BatchNormalization()(encoder)\n",
        "encoder = tf.keras.layers.Dense(64, activation='relu')(encoder)\n",
        "encoder = tf.keras.layers.BatchNormalization()(encoder)\n",
        "\n",
        "decoder = tf.keras.layers.Dense(64, activation='relu')(encoder)\n",
        "decoder = tf.keras.layers.BatchNormalization()(decoder)\n",
        "decoder = tf.keras.layers.Dense(226800, activation='sigmoid')(decoder)\n",
        "\n",
        "autoencoder10 = tf.keras.models.Model(inputs=input_img, outputs=decoder)\n",
        "autoencoder10.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-3), loss=tf.keras.losses.MSE, metrics = [RootMeanSquaredError()]) #loss=tf.keras.losses.MSE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X7LfCendXMz"
      },
      "outputs": [],
      "source": [
        "# Train your model\n",
        "X_train_flattend = np.reshape(X_train, (len(X_train), 226800))\n",
        "X_valid_flattend = np.reshape(X_valid, (len(X_valid), 226800))\n",
        "y_train_flattend = np.reshape(y_train, (len(y_train), 226800))\n",
        "y_valid_flattend = np.reshape(y_valid, (len(y_valid), 226800))\n",
        "\n",
        "X_test_flattend  = np.reshape(X_test, (len(X_test), 226800))\n",
        "\n",
        "history10 = autoencoder10.fit(X_train_flattend,y_train_flattend,\n",
        "                      epochs=100,\n",
        "                      batch_size=5,\n",
        "                      validation_data=(X_valid_flattend, y_valid_flattend))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acdor9c2el1H"
      },
      "outputs": [],
      "source": [
        "# Compute the prediction\n",
        "predicted_label = autoencoder10.predict(X_test_flattend)\n",
        "print_recontructed_text(X_test, predicted_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNQsH2LvfJvW"
      },
      "source": [
        "# Basic CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhDYUvzxfN06"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "\n",
        "input_img = tf.keras.layers.Input(shape=(420, 540, 1)) # adapt this if using `channels_first` image data format\n",
        "\n",
        "encoded = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(input_img)\n",
        "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(encoded)\n",
        "encoded = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(encoded)\n",
        "\n",
        "decoded = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
        "decoded = tf.keras.layers.UpSampling2D((2, 2))(decoded)\n",
        "decoded = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = tf.keras.layers.UpSampling2D((2, 2))(decoded)\n",
        "decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(decoded)\n",
        "\n",
        "autoencoder11 = tf.keras.models.Model(inputs = input_img, outputs = decoded)\n",
        "autoencoder11.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-2),\n",
        "                      loss='mse',\n",
        "                      metrics = [RootMeanSquaredError()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWCCbW8mgGjn"
      },
      "outputs": [],
      "source": [
        "autoencoder11.summary()\n",
        "\n",
        "history11 = autoencoder11.fit(X_train, y_train,\n",
        "                  epochs=100,\n",
        "                  batch_size=3,\n",
        "                  validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6JvGDtWglLv"
      },
      "outputs": [],
      "source": [
        "\n",
        "predicted_label = np.squeeze(autoencoder11.predict(X_test))\n",
        "print_recontructed_text(X_test, predicted_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvedFlXTgwpz"
      },
      "source": [
        "# Complex CNN 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmCxG5V1g7j7"
      },
      "outputs": [],
      "source": [
        "\n",
        "input_img = tf.keras.layers.Input(shape=(420, 540, 1)) # adapt this if using `channels_first` image data format\n",
        "encoded = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "encoded = tf.keras.layers.BatchNormalization()(encoded)\n",
        "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(encoded)\n",
        "encoded = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = tf.keras.layers.BatchNormalization()(encoded)\n",
        "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(encoded)\n",
        "\n",
        "decoded = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(encoded)\n",
        "decoded = tf.keras.layers.BatchNormalization()(decoded)\n",
        "decoded = tf.keras.layers.UpSampling2D((2, 2))(decoded)\n",
        "decoded = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = tf.keras.layers.BatchNormalization()(decoded)\n",
        "decoded = tf.keras.layers.UpSampling2D((2, 2))(decoded)\n",
        "decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(decoded)\n",
        "\n",
        "autoencoder12 = tf.keras.models.Model(inputs = input_img, outputs = decoded)\n",
        "autoencoder12.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
        "                    loss='mse',\n",
        "                    metrics=[RootMeanSquaredError()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0R2kA0zhME9"
      },
      "outputs": [],
      "source": [
        "\n",
        "autoencoder12.summary()\n",
        "history12 = autoencoder12.fit(X_train, y_train, epochs=100, batch_size=5, validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5mBQQ2thQZf"
      },
      "outputs": [],
      "source": [
        "\n",
        "predicted_label = np.squeeze(autoencoder12.predict(X_test))\n",
        "print_recontructed_text(X_test, predicted_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd7yM-Mxg1Bv"
      },
      "source": [
        "# Complex CNN 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaBRX4RChR5p"
      },
      "outputs": [],
      "source": [
        "\n",
        "input_img = tf.keras.layers.Input(shape=(420, 540, 1)) # adapt this if using `channels_first` image data format\n",
        "encoded = tf.keras.layers.Conv2D(48, (3, 3), activation='relu', padding='same')(input_img)\n",
        "encoded = tf.keras.layers.Conv2D(72, (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = tf.keras.layers.Conv2D(144, (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = tf.keras.layers.BatchNormalization()(encoded)\n",
        "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(encoded)\n",
        "\n",
        "encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
        "\n",
        "\n",
        "decoded = tf.keras.layers.Conv2D(144, (3, 3), activation='relu', padding='same')(encoded)\n",
        "decoded = tf.keras.layers.Conv2D(72, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = tf.keras.layers.Conv2D(48, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = tf.keras.layers.BatchNormalization()(decoded)\n",
        "decoded = tf.keras.layers.UpSampling2D((2, 2))(decoded)\n",
        "\n",
        "decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(decoded)\n",
        "\n",
        "autoencoder13 = tf.keras.models.Model(inputs = input_img, outputs = decoded)\n",
        "autoencoder13.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-3), loss='mse', metrics=[RootMeanSquaredError()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0A3KE9chkZk"
      },
      "outputs": [],
      "source": [
        "autoencoder13.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnW1Fd1uhnch"
      },
      "outputs": [],
      "source": [
        "history13 = autoencoder13.fit(X_train, y_train,\n",
        "                  epochs=100,\n",
        "                  batch_size=10,\n",
        "                  validation_data=(X_valid, y_valid),\n",
        "                  shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdWHjpeHhq_q"
      },
      "outputs": [],
      "source": [
        "\n",
        "predicted_label = np.squeeze(autoencoder13\n",
        "                             .predict(X_test))\n",
        "print_recontructed_text(X_test, predicted_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFcpvY2U6NVw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmwqdzRSg34I"
      },
      "source": [
        "# Summary of section 3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Model 10 = Dense 128, 64, 128, Adam, le-2, loss 0.17\n",
        "2. Model 11 = CNN 128, 64, Maxpool, Adam, loss 0.28\n",
        "3. Model 12 = CNN 64, 128, Maxpool, Batch norm, Adam, loss 0.04\n",
        "4. Model 13 = 48, 72, 144, Maxpool, batch norm, dropout, 0.02"
      ],
      "metadata": {
        "id": "DGg_J59RkBnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. In case of text recontruction the dense models do not perform good as CNN's\n",
        "2.  In a dense model, each input feature is processed independently, without regard for its position in the sequence. This makes it difficult for the model to capture the complex relationships between different words or characters in the input sequence, which are crucial for generating accurate reconstructions.\n",
        "3. Basic CNN's  will require a large number of filters and layers to achieve good performance on text data, which can increase the risk of overfitting. To overcome that, in the complex CNN's I have used Maxpool layer.\n",
        "4. Also the best accuracy is when we add dropout layer to the CNN's"
      ],
      "metadata": {
        "id": "0Tsympx9k1Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_plot(history10, \"Dense 128, 64, 128, Adam, le-2, loss 0.17\")\n",
        "loss_plot(history11, \"CNN 128, 64, Maxpool, Adam, loss 0.28\")\n",
        "loss_plot(history12, \"CNN 64, 128, Maxpool, Batch norm, Adam, loss 0.04\")\n",
        "loss_plot(history13, \"48, 72, 144, Maxpool, batch norm, dropout, 0.02\")\n"
      ],
      "metadata": {
        "id": "vq5ae-jLbjcq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}